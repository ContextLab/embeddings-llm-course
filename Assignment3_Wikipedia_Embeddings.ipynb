{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextLab/embeddings-llm-course/blob/main/Assignment3_Wikipedia_Embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Representing Meaning - A Computational Exploration of Semantic Space\n",
    "\n",
    "**PSYC 51.17: Models of Language and Communication**\n",
    "\n",
    "**Due Date: January 30, 2026 at 11:59 PM EST**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "*\"You shall know a word by the company it keeps.\"* - J.R. Firth, 1957\n",
    "\n",
    "In this assignment, you will explore how machines represent meaning using 250,000 Wikipedia articles. You will implement and compare methods spanning five decades of computational linguistics - from classical statistical techniques (LSA, LDA) to modern large language models (BERT, GPT-2, Sentence Transformers, Llama).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By completing this assignment, you will:\n",
    "- Understand the evolution of semantic representation from classical to modern NLP\n",
    "- Implement and compare traditional, neural, and LLM-based embedding methods\n",
    "- Develop expertise in clustering evaluation and unsupervised learning\n",
    "- Connect computational methods to cognitive theories of semantic memory\n",
    "- Create publication-quality visualizations of high-dimensional semantic spaces\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#1-setup-and-installation)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Part 1: Implementing the Embedding Zoo](#3-part-1-implementing-the-embedding-zoo)\n",
    "   - 1.1 Classical Statistical Methods (LSA, LDA)\n",
    "   - 1.2 Static Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "   - 1.3 Contextualized Embeddings (BERT, GPT-2)\n",
    "   - 1.4 Modern Sentence/Document Embeddings\n",
    "   - 1.5 Modern Topic Models (BERTopic, Top2Vec)\n",
    "4. [Part 2: Evaluation and Analysis](#4-part-2-evaluation-and-analysis)\n",
    "5. [Part 3: Visualization](#5-part-3-visualization)\n",
    "6. [Part 4: Cognitive Science Connection](#6-part-4-cognitive-science-connection)\n",
    "7. [Part 5: Advanced Extensions](#7-part-5-advanced-extensions)\n",
    "8. [Conclusion](#8-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers sentence-transformers torch\n",
    "!pip install -q gensim scikit-learn umap-learn hdbscan\n",
    "!pip install -q bertopic top2vec\n",
    "!pip install -q plotly matplotlib seaborn\n",
    "!pip install -q pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import pickle\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Wikipedia dataset if it doesn't exist\n",
    "dataset_url = 'https://www.dropbox.com/s/v4juxkc5v2rd0xr/wikipedia.pkl?dl=1'\n",
    "dataset_path = 'wikipedia.pkl'\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"Downloading dataset (this may take a few minutes)...\")\n",
    "    urllib.request.urlretrieve(dataset_url, dataset_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    wikipedia = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(wikipedia):,} Wikipedia articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(f\"Type: {type(wikipedia)}\")\n",
    "print(f\"Number of articles: {len(wikipedia):,}\")\n",
    "print(f\"\\nFirst article keys: {wikipedia[0].keys()}\")\n",
    "print(f\"\\nExample article:\")\n",
    "print(f\"  Title: {wikipedia[0]['title']}\")\n",
    "print(f\"  Text length: {len(wikipedia[0]['text'])} characters\")\n",
    "print(f\"  ID: {wikipedia[0]['id']}\")\n",
    "print(f\"  URL: {wikipedia[0]['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development, start with a subset (uncomment for full dataset)\n",
    "# You can start with 10K articles for faster iteration, then scale up\n",
    "SAMPLE_SIZE = 10000  # Change to len(wikipedia) for full dataset\n",
    "\n",
    "if SAMPLE_SIZE < len(wikipedia):\n",
    "    print(f\"Using subset of {SAMPLE_SIZE:,} articles for development\")\n",
    "    sample_indices = random.sample(range(len(wikipedia)), SAMPLE_SIZE)\n",
    "    articles = [wikipedia[i] for i in sample_indices]\n",
    "else:\n",
    "    articles = wikipedia\n",
    "    print(f\"Using full dataset of {len(articles):,} articles\")\n",
    "\n",
    "# Extract texts and titles for convenience\n",
    "texts = [article['text'] for article in articles]\n",
    "titles = [article['title'] for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part 1: Implementing the Embedding Zoo (40 points)\n",
    "\n",
    "In this section, you will implement 10+ different embedding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Classical Statistical Methods (8 points)\n",
    "\n",
    "Implement Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement LSA using TF-IDF + TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "# Your LSA implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement LDA\n",
    "# Your LDA implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Static Word Embeddings (8 points)\n",
    "\n",
    "Implement Word2Vec, GloVe, and FastText with document-level aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Word2Vec, GloVe, FastText embeddings\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained models\n",
    "# word2vec_model = api.load('word2vec-google-news-300')\n",
    "# glove_model = api.load('glove-wiki-gigaword-300')\n",
    "# fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Contextualized Embeddings (8 points)\n",
    "\n",
    "Implement BERT and GPT-2 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement BERT embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Your BERT implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement GPT-2 embeddings\n",
    "# Your GPT-2 implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Modern Sentence/Document Embeddings (8 points)\n",
    "\n",
    "Implement Sentence Transformers and optionally Llama embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Modern Topic Models (8 points)\n",
    "\n",
    "Implement BERTopic and Top2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement BERTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Top2Vec\n",
    "# from top2vec import Top2Vec\n",
    "\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part 2: Evaluation and Analysis (30 points)\n",
    "\n",
    "Apply clustering algorithms and compute comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement clustering (K-Means, Hierarchical, DBSCAN/HDBSCAN)\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "import hdbscan\n",
    "\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement evaluation metrics\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Part 3: Visualization (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement dimensionality reduction and visualization\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Part 4: Cognitive Science Connection (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributional Semantics and Cognitive Science\n",
    "\n",
    "*Write your analysis here (2-3 pages) connecting your computational work to theories of meaning in cognitive science.*\n",
    "\n",
    "TODO: Your essay here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Part 5: Advanced Extensions (5 points)\n",
    "\n",
    "Choose at least ONE extension: Cross-Lingual, Temporal Analysis, or Practical Applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your chosen extension\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "*Summarize your findings, discuss limitations, and suggest future directions.*\n",
    "\n",
    "TODO: Your conclusion here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}